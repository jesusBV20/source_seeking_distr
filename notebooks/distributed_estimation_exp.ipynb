{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "# Copyright (C) 2024 Jes√∫s Bautista Villar <jesbauti20@gmail.com>\n",
    "- Distributed estimation of the centroid and the ascending direction -\n",
    "\"\"\"\n",
    "!python -V || python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If executed from Google Colab --------------------------------------------------------\n",
    "# !git clone https://github.com/jesusBV20/source_seeking_distr.git\n",
    "# !rsync -a source_seeking_distr/ .\n",
    "# !rm -r source_seeking_distr\n",
    "\n",
    "# If you want to use latex with matplotlib ---------------------------------------------\n",
    "# !apt install -y texlive texlive-latex-extra texlive-fonts-recommended dvipng cm-super\n",
    "# !pip install -y latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory '../output' already exists!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Graphic tools\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import PillowWriter, FFMpegWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Scalar field functions\n",
    "from sourceseeking_distr.scalar_field.sigma_funcs import SigmaGauss, SigmaNonconvex, SigmaFract \n",
    "\n",
    "# Main utility functions used along the notebook\n",
    "from sourceseeking_distr.toolbox.math_utils import XY_distrib, gen_random_graph, gen_Z_split\n",
    "from sourceseeking_distr.toolbox.basic_utils import createDir\n",
    "from sourceseeking_distr.toolbox.plot_utils import kw_def_patch, kw_def_arrow, unicycle_patch, vector2d\n",
    "from sourceseeking_distr.toolbox.math_utils import unit_vec, L_sigma\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Create the output directory\n",
    "OUTPUT_FOLDER = os.path.join(\"..\",\"output\")\n",
    "createDir(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSconSim:\n",
    "    def __init__(self, k_asc, k_cen, device_n, iter_n):\n",
    "        self.device_n = device_n\n",
    "        self.k_asc = k_asc\t\n",
    "        self.k_cen = k_cen\n",
    "        self.iter_n = iter_n\n",
    "        self.com_graph = np.array([[0,1,0],\n",
    "                                   [1,0,1],\n",
    "                                   [0,1,0]])\n",
    "  \n",
    "    def calc_asc_at_step(self, steps, centroids, sigmas):\n",
    "        if (any(steps) > self.iter_n+1 or any(steps) < 0):\n",
    "            steps = [0]*len(steps)\n",
    "        step = max(steps)+1\n",
    "        \n",
    "        assert(len(centroids) == self.device_n and len(sigmas) == self.device_n and step >= 0)\n",
    "        \n",
    "        asc_dirs = np.zeros((step, self.device_n, self.device_n, 2))\n",
    "        \n",
    "        for dev in range(self.device_n):\n",
    "            asc_dirs[0, dev, dev] = centroids[dev] * sigmas[dev]\n",
    "        \n",
    "        for i in range(step):\n",
    "            prev_asc_dirs = asc_dirs[i-1].copy() if i >0 else asc_dirs[0].copy()\n",
    "            for s_dev in range(self.device_n):\n",
    "                for p_dev in range(self.device_n):\n",
    "                    if (self.com_graph[s_dev, p_dev] == 1):\n",
    "                        asc_dirs[i, s_dev, p_dev] = prev_asc_dirs[p_dev, p_dev]\n",
    "                    else:\n",
    "                        asc_dirs[i, s_dev, p_dev] = prev_asc_dirs[s_dev, s_dev]\n",
    "                \n",
    "                asc_dir_sum = np.zeros((2))    \n",
    "                for dev in range(self.device_n):\n",
    "                    asc_dir_sum += asc_dirs[i, s_dev, s_dev] - asc_dirs[i, s_dev,dev]\n",
    "                    \n",
    "                asc_dirs[i, s_dev, s_dev] += -self.k_asc * asc_dir_sum\n",
    "        \n",
    "        asc_dirs_ret = np.zeros((self.device_n, 2))\n",
    "        \n",
    "        for s_dev in range(self.device_n):\n",
    "            asc_dirs_ret[s_dev] = asc_dirs[steps[s_dev], s_dev, s_dev]\n",
    "            \n",
    "        # Normalize asc_dirs_ret to unit vectors\n",
    "        asc_dirs_ret = asc_dirs_ret / np.linalg.norm(asc_dirs_ret, axis=1, keepdims=True)\n",
    "        \n",
    "        return asc_dirs_ret\n",
    "    \n",
    "    def calc_cen_at_step(self, steps, pos):\n",
    "        if (any(steps) > self.iter_n+1 or any(steps) < 0):\n",
    "            steps = [0]*len(steps)\n",
    "        step = max(steps)+1\n",
    "        \n",
    "        assert(len(pos) == self.device_n and step >= 0)\n",
    "        \n",
    "        centroids = np.zeros((step, self.device_n, self.device_n, 2))\n",
    "        \n",
    "        pos_sum = np.zeros((self.device_n, 2))\n",
    "        for s_dev in range(self.device_n):\n",
    "            for p_dev in range(self.device_n):\n",
    "                pos_sum[s_dev] += pos[s_dev] - pos[p_dev]\n",
    "        \n",
    "        for s_dev in range(self.device_n):\n",
    "            cen_sum = np.zeros((2))    \n",
    "            for dev in range(self.device_n):\n",
    "                cen_sum += centroids[0, s_dev, s_dev] - centroids[0, s_dev,dev]\n",
    "            centroids[0, s_dev, s_dev] += -self.k_cen * (cen_sum - pos_sum[s_dev])\n",
    "        \n",
    "        for i in range(step):\n",
    "            prev_centroids = centroids[i-1].copy() if i >0 else centroids[0].copy()\n",
    "            for s_dev in range(self.device_n):\n",
    "                for p_dev in range(self.device_n):\n",
    "                    if (self.com_graph[s_dev][p_dev] == 1):\n",
    "                        centroids[i, s_dev, p_dev] = prev_centroids[p_dev, p_dev]\n",
    "                    else:\n",
    "                        centroids[i, s_dev, p_dev] = prev_centroids[s_dev, s_dev] - (pos[s_dev] - pos[p_dev])\n",
    "                        \n",
    "                cen_sum = np.zeros((2))\n",
    "                for dev in range(self.device_n):\n",
    "                    cen_sum += centroids[i, s_dev, s_dev] - centroids[i, s_dev,dev]\n",
    "                centroids[i, s_dev, s_dev] += -self.k_cen * (cen_sum - pos_sum[s_dev])\n",
    "        \n",
    "        centroids_ret = np.zeros((self.device_n, 2))\n",
    "        for s_dev in range(self.device_n):\n",
    "            centroids_ret[s_dev] = centroids[steps[s_dev], s_dev, s_dev]\n",
    "        \n",
    "        return centroids_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "from matplotlib.legend import Legend # legend artist\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def build_B(list_edges, n):\n",
    "    B = np.zeros((n,len(list_edges)))\n",
    "    for i in range(len(list_edges)):\n",
    "        B[list_edges[i][0]-1, i] = 1\n",
    "        B[list_edges[i][1]-1, i] = -1\n",
    "    return B\n",
    "\n",
    "def build_L_from_B(B):\n",
    "    L = B@B.T\n",
    "    return L\n",
    "\n",
    "def dyn_centroid_estimation(xhat_0, t, Lb, x, k=1):\n",
    "    xhat_dt = - k*(Lb.dot(xhat_0) - Lb.dot(x))\n",
    "    return xhat_dt\n",
    "\n",
    "def anim_centroid_estimation(ss_npa, Z, N, fps=30):\n",
    "    \"\"\"\n",
    "    Funtion to animate the centroid estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    P = ss_npa[\"DWSS_DATA:pos\"]\n",
    "    S = ss_npa[\"DWSS_DATA:sigma\"]\n",
    "    C = ss_npa[\"DWSS_DATA:centroid\"]\n",
    "    mu = ss_npa[\"DWSS_DATA:asc_dir_norm\"]\n",
    "    \n",
    "    s_max = np.max(S[:,:])\n",
    "    s_min = np.min(S[:,:][S>1e-5])\n",
    "    \n",
    "    p_max = np.array([np.max(P[:,:,0]),np.max(P[:,:,1])])\n",
    "    p_min = np.array([np.min(P[:,:,0]),np.min(P[:,:,1])])\n",
    "    \n",
    "    # Build the Laplacian matrix\n",
    "    B = build_B(Z,N)\n",
    "    L = build_L_from_B(B)\n",
    "\n",
    "    # Compute algebraic connectivity (lambda_2)\n",
    "    eig_vals = np.linalg.eigvals(L)\n",
    "    min_eig_val = np.min(eig_vals[eig_vals > 1e-7])\n",
    "\n",
    "    # Simulation -------------------------------------------------------\n",
    "    \n",
    "    pc_hat = P-C\n",
    "        \n",
    "    # S = -1*S[:,:] + 2*s_max\n",
    "        \n",
    "    # Precalculated ----------------------------------------------------\n",
    "    \n",
    "    pc_comp = np.average(P, axis=1)\n",
    "    \n",
    "    pc_sim = np.zeros_like(pc_hat)\n",
    "    mu_sim = np.zeros_like(mu)\n",
    "    \n",
    "    sim = SSconSim(ss_npa[\"DWSS_DATA:k_asc\"][0,0], ss_npa[\"DWSS_DATA:k_cen\"][0,0], N, ss_npa[\"DWSS_DATA:iter_n\"][0,0])\n",
    "        \n",
    "    for i, data_point in enumerate(P):\n",
    "        steps = ss_npa[\"DWSS_DATA:step\"][i]\n",
    "        pc_sim[i] = sim.calc_cen_at_step(steps, data_point)\n",
    "        mu_sim[i] = sim.calc_asc_at_step(steps, ss_npa[\"DWSS_DATA:last_centroid\"][i], ss_npa[\"DWSS_DATA:sigma\"][i])\n",
    "        \n",
    "    pc_sim = P-pc_sim\n",
    "    \n",
    "    mu_field = np.array([5,25])\n",
    "    A = np.eye(2)\n",
    "    sigma_test = SigmaGauss(mu=mu_field, max_intensity=2000, dev=30)\n",
    "    sigma_test.A = A\n",
    "    \n",
    "    # S_comp = np.apply_along_axis(sigma_test.eval_value, axis=2, arr=P)\n",
    "    # S_comp = S_comp[:,:,0]\n",
    "     \n",
    "    mu_err = np.arccos(np.einsum('ijk,ijk->ij', mu, mu_sim))\n",
    "    c_err = pc_hat - np.broadcast_to(pc_comp[:, np.newaxis, :], pc_hat.shape)\n",
    "    c_err = np.linalg.norm(c_err, axis=2)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # -- Error plot --\n",
    "    fig_err = plt.figure()\n",
    "    ax_mu_err, ax_c_err = fig_err.subplots(2,1)\n",
    "\n",
    "    # Axis configuration\n",
    "    ax_c_err.grid(True)\n",
    "    ax_mu_err.grid(True)\n",
    "\n",
    "    ax_c_err.set_xlabel(\"$T$\")\n",
    "    ax_c_err.set_ylabel(\"Centroid error (m)\")\n",
    "    ax_mu_err.set_xlabel(\"$T$\")\n",
    "    ax_mu_err.set_ylabel(\"L error(rad)\")\n",
    "\n",
    "    # Lines\n",
    "    ax_c_err.axhline(0, c=\"k\", ls=\"-\", lw=1.1)\n",
    "    ax_c_err.axvline(0, c=\"k\", ls=\"-\", lw=1.1)\n",
    "    ax_mu_err.axhline(0, c=\"k\", ls=\"-\", lw=1.1)\n",
    "    ax_mu_err.axhline(np.pi/2, c=\"k\", ls=\"-\", lw=1.1)\n",
    "    ax_mu_err.axvline(0, c=\"k\", ls=\"-\", lw=1.1)\n",
    "\n",
    "    for i in range(P.shape[1]):\n",
    "        ax_c_err.plot(c_err[:,i], label=i)\n",
    "        ax_mu_err.plot(mu_err[:,i], label=i)\n",
    "\n",
    "        ax_c_err.legend()\n",
    "\n",
    "    # -- Animation --\n",
    "    fig = plt.figure()\n",
    "    ax = fig.subplots()\n",
    "    \n",
    "    # ax.axis([p_min[0]-ds, p_max[0]+ds, p_min[1]-ds, p_max[1]+ds])\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    title = \"SS\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(\"$Y$ [L]\")\n",
    "    ax.set_ylabel(\"$X$ [L]\")\n",
    "\n",
    "    # Lines\n",
    "    ax.axhline(0, c=\"k\", ls=\"-\", lw=1.1)\n",
    "    ax.axvline(0, c=\"k\", ls=\"-\", lw=1.1)\n",
    "    \n",
    "    # sigma_test.draw(fig=fig, ax=ax, xlim=70, ylim=40, n=300, contour_levels=20)\n",
    "\n",
    "    # Centroids\n",
    "    pts = ax.scatter(pc_hat[0,:,0], pc_hat[0,:,1], color=\"r\", marker=r\"$x$\", s=50)\n",
    "    pts_sim = ax.scatter(pc_sim[0,:,0], pc_sim[0,:,1], color=\"y\", marker=r\"$x$\", s=50)\n",
    "    pts_comp = ax.scatter(pc_comp[0,0], pc_comp[0,1], color=\"b\", marker=r\"$x$\", s=50)\n",
    "\n",
    "    # Set up the normalization and colormap\n",
    "    color_norm = mcolors.Normalize(vmin=s_min, vmax=s_max)\n",
    "    cmap = plt.cm.get_cmap('RdYlGn_r')  # 'RdYlGn_r' is green to red\n",
    "    \n",
    "    # Agents\n",
    "    agts = ax.scatter(P[0,:,0], P[0,:,1], c=S[0,:], s=100, cmap=cmap, norm=color_norm)\n",
    "            \n",
    "    # Estimated ascending direction mu\n",
    "    quivers = ax.quiver(P[0,:,0], P[0,:,1], mu[0,:,0], mu[0,:,1], color=\"r\", angles='xy', scale_units='xy', scale=1)\n",
    "    \n",
    "    # Ascending direction computed from centroid data\n",
    "    quivers_comp = ax.quiver(P[0,:,0], P[0,:,1], mu_sim[0,:,0], mu_sim[0,:,1], color=\"b\", angles='xy', scale_units='xy', scale=1)\n",
    "    \n",
    "    agt_edges = []    \n",
    "    for edge in Z:\n",
    "        agt_edge, = ax.plot([P[0,edge[0]-1,0], P[0,edge[1]-1,0]], [P[0,edge[0]-1,1], P[0,edge[1]-1,1]], \"k--\")\n",
    "        agt_edges.append(agt_edge)\n",
    "\n",
    "    # -- Building the animation --\n",
    "    anim_frames = C.shape[0]\n",
    "    ann_list = []\n",
    "\n",
    "    # Function to update the animation\n",
    "    def animate(i):\n",
    "        p_max = np.array([np.max(P[i,:,0]),np.max(P[i,:,1])])\n",
    "        p_min = np.array([np.min(P[i,:,0]),np.min(P[i,:,1])])\n",
    "        \n",
    "        # Add padding (optional)\n",
    "        padding = 0.8  # 10% padding\n",
    "        x_range = p_max[0] - p_min[0]\n",
    "        y_range = p_max[1] - p_min[1]\n",
    "        lim_range = max(x_range, y_range)\n",
    "        \n",
    "        ax.set_xlim(p_min[0] - padding * lim_range, p_max[0] + padding * lim_range)\n",
    "        ax.set_ylim(p_min[1] - padding * lim_range, p_max[1] + padding * lim_range)\n",
    "        \n",
    "        v_scale = np.linalg.norm(p_max-p_min)/3\n",
    "        \n",
    "        for ann in ann_list:\n",
    "            ann.remove()\n",
    "        ann_list.clear()\n",
    "        \n",
    "        # Update the centroid estimation markers\n",
    "        pts.set_offsets(pc_hat[i])\n",
    "        pts_sim.set_offsets(pc_sim[i])\n",
    "        pts_comp.set_offsets(pc_comp[i])\n",
    "        agts.set_offsets(P[i])\n",
    "        agts.set_array(S[i,:])\n",
    "        for agt_edge, edge in zip(agt_edges, Z):\n",
    "            agt_edge.set_data([P[i,edge[0]-1,0], P[i,edge[1]-1,0]], [P[i,edge[0]-1,1], P[i,edge[1]-1,1]])\n",
    "        \n",
    "        quivers.set_offsets(P[i])\n",
    "        quivers.set_UVC(mu[i,:,0]*v_scale, mu[i,:,1]*v_scale)\n",
    "        \n",
    "        quivers_comp.set_offsets(P[i])\n",
    "        quivers_comp.set_UVC(mu_sim[i,:,0]*v_scale, mu_sim[i,:,1]*v_scale)\n",
    "        \n",
    "        # Step number\n",
    "        for j, (x, y) in enumerate(P[i,:,:]):\n",
    "            ann_list.append(ax.annotate(f\"{j}:{ss_npa[\"DWSS_DATA:step\"][i,j]}\", (x, y), \n",
    "                        textcoords=\"offset points\", xytext=(0,10), ha='center'))\n",
    "\n",
    "    # Generate the animation\n",
    "    print(\"Simulating {0:d} frames...\".format(anim_frames))\n",
    "    anim = FuncAnimation(fig, animate, frames=tqdm(range(anim_frames), initial=1, position=0), \n",
    "                         interval=1/fps*1000)\n",
    "    anim.embed_limit = 40\n",
    "\n",
    "    # Close plots and return the animation class to be compiled\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSReader:\n",
    "    CSV_FIELDS = (\"Time\", \"DWSS_DATA:step\", \"DWSS_DATA:last_pos\", \"DWSS_DATA:centroid\", \"DWSS_DATA:last_centroid\", \n",
    "              \"DWSS_DATA:asc_dir\", \"DWSS_DATA:asc_dir_norm\", \"DWSS_DATA:pos\", \"DWSS_DATA:sigma\", \"DWSS_DATA:iter_n\", \n",
    "              \"DWSS_DATA:k_cen\", \"DWSS_DATA:k_asc\", \"DWSS_DATA:field_offset\")\n",
    "\n",
    "    def __init__(self, dirpath: str):\n",
    "        self.dirpath = dirpath\n",
    "        self.data_dict = {}\n",
    "        self.min_len = 9999999\n",
    "        self.load_all_csv_in_folder()\n",
    "\n",
    "    def parse_value(self, value):\n",
    "        \"\"\"\n",
    "        Parse a string value as either a tuple of numbers or a single float.\n",
    "        Returns default values in case of parsing errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the value contains a comma, indicating a point in space\n",
    "            if ',' in value:\n",
    "                # Attempt to parse as a tuple\n",
    "                parsed_value = np.array(ast.literal_eval(f'({value})'))\n",
    "                return parsed_value\n",
    "            elif '.' in value:\n",
    "                # Attempt to parse as a float\n",
    "                return float(value)\n",
    "            else:\n",
    "                return int(value)\n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            # Return default values in case of errors\n",
    "            if ',' in value:\n",
    "                return np.array([0.0, 0.0])  # Default for tuple values\n",
    "            else:\n",
    "                return 0.0  # Default for single float values\n",
    "\n",
    "    def load_csv_to_dict(self, filename):\n",
    "        \"\"\"Load a CSV file into a dictionary with lists of parsed values.\"\"\"\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Open the CSV file with tab delimiter\n",
    "        with open(filename, mode='r') as csvfile:\n",
    "            csvreader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "            \n",
    "            # Fill the dictionary with parsed data\n",
    "            for row in csvreader:\n",
    "                for header in csvreader.fieldnames:\n",
    "                    if header in self.CSV_FIELDS:\n",
    "                        if header not in data_dict:\n",
    "                            data_dict[header] = []\n",
    "                        data_dict[header].append(self.parse_value(row[header]))\n",
    "        \n",
    "        return data_dict\n",
    "\n",
    "    def convert_dict_to_numpy_arrays(self, data_dict):\n",
    "        \"\"\"Convert lists in the dictionary to appropriate NumPy arrays or keep them as lists if necessary.\"\"\"\n",
    "        numpy_arrays = {}\n",
    "        for key, value in data_dict.items():\n",
    "            # Check if the list contains only scalars\n",
    "            if all(isinstance(v, (float, int)) for v in value):\n",
    "                numpy_arrays[key] = np.array(value)\n",
    "            else:\n",
    "                numpy_arrays[key] = np.vstack(value)\n",
    "                \n",
    "            if len(numpy_arrays[key]) < self.min_len:\n",
    "                self.min_len = len(numpy_arrays[key])\n",
    "        \n",
    "        return numpy_arrays\n",
    "\n",
    "    def load_all_csv_in_folder(self):\n",
    "        # List all CSV files in the given folder\n",
    "        for file_name in os.listdir(self.dirpath):\n",
    "            if file_name.endswith('.csv'):\n",
    "                # Get the full file path\n",
    "                file_path = os.path.join(self.dirpath, file_name)\n",
    "                \n",
    "                # Load CSV data into a dictionary\n",
    "                data_dict = self.load_csv_to_dict(file_path)\n",
    "                \n",
    "                # Convert the dictionary to appropriate NumPy arrays or lists\n",
    "                numpy_arrays = self.convert_dict_to_numpy_arrays(data_dict)\n",
    "                \n",
    "                # Store the result in the self.data_dict dictionary, using the file name (without extension) as the key\n",
    "                \n",
    "                file_date, file_id = self.extract_date_and_id_from_filename(file_name)\n",
    "                    \n",
    "                if file_date not in self.data_dict:\n",
    "                    self.data_dict[file_date] = {}\n",
    "                \n",
    "                if file_id not in self.data_dict[file_date]:\n",
    "                    self.data_dict[file_date][file_id] = {}\n",
    "                    \n",
    "                self.data_dict[file_date][file_id] = numpy_arrays\n",
    "                \n",
    "    def extract_date_and_id_from_filename(self, filename):\n",
    "        \"\"\"Extract the date and ID from the filename in the format [date]_[time]_[id].csv.\"\"\"\n",
    "        match = re.match(r\"(\\d{2}_\\d{2}_\\d{2}__\\d{2}_\\d{2}_\\d{2})_(\\d+)\", filename)\n",
    "        if match:\n",
    "            date = match.group(1)\n",
    "            file_id = match.group(2)\n",
    "            return date, file_id\n",
    "        return None, None\n",
    "    \n",
    "    def align_dicts(self, timestamp_col='Time'):\n",
    "        \n",
    "        for file_date, devices in self.data_dict.items():\n",
    "            for dev, dev_data in devices.items():\n",
    "                self.data_dict[file_date][dev] = pd.DataFrame({col_name: list(dev_cols) for col_name, dev_cols in dev_data.items()})\n",
    "                \n",
    "            start_timestamp = max(df[timestamp_col].iloc[0] for df in devices.values())\n",
    "            all_timestamps = pd.concat([df[timestamp_col] for df in devices.values()]).unique()\n",
    "            all_timestamps = sorted(ts for ts in all_timestamps if ts >= start_timestamp)\n",
    "            \n",
    "            for dev, dev_data in devices.items():\n",
    "                df = dev_data.sort_values(by=timestamp_col).reset_index(drop=True)\n",
    "                full_df = pd.DataFrame({timestamp_col: all_timestamps})  # DataFrame with all timestamps\n",
    "                \n",
    "                # Merge original dataframe with the full timestamp dataframe\n",
    "                merged_df = pd.merge_asof(\n",
    "                    full_df, \n",
    "                    df, \n",
    "                    on=timestamp_col, \n",
    "                    direction='backward'  # Fill missing timestamps with the last available row\n",
    "                )\n",
    "                \n",
    "                # print(merged_df)\n",
    "                \n",
    "                self.data_dict[file_date][dev] = {col: np.array(merged_df[col].tolist()) for col in merged_df.columns}\n",
    "                \n",
    "            self.min_len = len(self.data_dict[file_date]['1'][self.CSV_FIELDS[0]])\n",
    "            for dev, dev_data in devices.items():\n",
    "                if len(dev_data[self.CSV_FIELDS[0]]) < self.min_len:\n",
    "                    self.min_len = len(dev_data[self.CSV_FIELDS[0]])\n",
    "                \n",
    "\n",
    "    def to_npa(self):\n",
    "        npa_dict = {}\n",
    "        for file_date, devices in self.data_dict.items():\n",
    "            new_dict = {}\n",
    "            for field_name in devices['1'].keys():\n",
    "                if field_name in self.CSV_FIELDS:\n",
    "                    # list_t = [dev[field_name][:self.min_len] for dev in devices.values()]\n",
    "                    # print(list_t)\n",
    "                    new_dict[field_name] = np.stack([dev[field_name][:self.min_len] for dev in devices.values()], axis=1)\n",
    "        \n",
    "        npa_dict[file_date] = new_dict    \n",
    "        return npa_dict     \n",
    "          \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data =  SSReader(\"../input_data/\")\n",
    "ss_data.align_dicts()\n",
    "ss_npa = ss_data.to_npa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows(data_dict, interval):\n",
    "    \"\"\"Removes rows from NumPy arrays in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        data_dict: A dictionary where values are NumPy arrays of shape (n, ...).\n",
    "        interval: An integer specifying the interval at which to remove rows.\n",
    "\n",
    "    Returns:\n",
    "        A new dictionary with the modified arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    new_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        indices_to_keep = np.arange(value.shape[0]) % interval == 0\n",
    "        new_dict[key] = value[indices_to_keep]\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_anim_dat = ss_npa['24_09_13__12_09_10']\n",
    "# to_anim_dat = remove_rows(to_anim_dat, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Z = ((1,2), (2,3))\n",
    "\n",
    "# anim1 = anim_centroid_estimation(p, Z, c, l, s)\n",
    "# HTML(anim1.to_html5_video())\n",
    "\n",
    "anim2 = anim_centroid_estimation(to_anim_dat, Z, 3, fps=5)\n",
    "# writer = FFMpegWriter(fps=5)\n",
    "# anim2.save(\"animation.mp4\", writer=writer)\n",
    "HTML(anim2.to_html5_video())\n",
    "\n",
    "# writer = PillowWriter(fps=15, bitrate=1800)\n",
    "# anim.save(os.path.join(OUTPUT_FOLDER, \"centroid1.gif\"),\n",
    "#         writer = writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
